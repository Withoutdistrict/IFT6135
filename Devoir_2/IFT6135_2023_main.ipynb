{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTv0D26B9W2h"
   },
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IdN5pnC8MMx-"
   },
   "source": [
    "This notebook is intended to produce the plots and figures for the report on Problem 1 of the practical. You should not run this notebook in Google Colab until you have finished constructing the correct solutions for transformer_solution.py and encoder_decoder_solution.py\n",
    "\n",
    "This notebook provides some limited commentary on several HuggingFace Features and toolage. You will use HuggingFace Datasets to load the Amazon Polarity dataset for sentiment analysis. The notebook will define a Bert tokenizer, collate functions, and then train and evaluate several models using the HuggingFace utilities mentioned above. Remember, the most crucial part here is running the experiments for the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qFHMMDtSwuW4",
    "outputId": "8db247e6-4231-4915-d44a-1c4e30c72766"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "#@title Mount your Google Drive\n",
    "# If you run this notebook locally or on a cluster (i.e. not on Google Colab)\n",
    "# you can delete this cell which is specific to Google Colab. You may also\n",
    "# change the paths for data/logs in Arguments below.\n",
    "% matplotlib inline\n",
    "% load_ext autoreload\n",
    "% autoreload 2\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oODLwt1QzgGa"
   },
   "outputs": [],
   "source": [
    "#@title Link your assignment folder & install requirements\n",
    "#@markdown Enter the path to the assignment folder in your Google Drive\n",
    "# If you run this notebook locally or on a cluster (i.e. not on Google Colab)\n",
    "# you can delete this cell which is specific to Google Colab. \n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "\n",
    "folder = \"\"  #@param {type:\"string\"}\n",
    "!ln -Ts \"$folder\" / content / assignment 2 > / dev / null\n",
    "\n",
    "# Add the assignment folder to Python path\n",
    "# if '/content/assignment' not in sys.path:\n",
    "#   sys.path.insert(0, '/content/assignment')\n",
    "\n",
    "# Check if CUDA is available\n",
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    warnings.warn('CUDA is not available.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dt3NTvpsy4Oc"
   },
   "source": [
    "### Running on GPU\n",
    "For this assignment, it will be necessary to run your experiments on GPU. To make sure the notebook is running on GPU, you can change the notebook settings with\n",
    "* (EN) `Edit > Notebook Settings`\n",
    "* (FR) `Modifier > ParamÃ¨tres du notebook`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLVSmv9HoMH5"
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "import json\n",
    "\n",
    "from typing import List, Dict, Union, Optional, Tuple\n",
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# !pip install -qqq datasets transformers --upgrade\n",
    "from datasets import Dataset\n",
    "import transformers\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "from transformer_solution import Transformer\n",
    "from encoder_decoder_solution import EncoderDecoder\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from transformers import AutoModel\n",
    "from encoder_decoder_solution import EncoderDecoder\n",
    "from transformer_solution import Transformer\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.random.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pGJeUTOyPEHR",
    "outputId": "6c1b2c20-f7c0-4c59-ea10-bb0f15651c3f"
   },
   "outputs": [],
   "source": [
    "dataset_train = load_dataset(\"amazon_polarity\", split=\"train[:10000]\", cache_dir=\"assignment/data\")\n",
    "dataset_test = load_dataset(\"amazon_polarity\", split=\"test[:1000]\", cache_dir=\"assignment/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wn4myFpBOmQ2",
    "outputId": "1f8c2418-667f-4beb-963e-465fe5e1a9fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "title: Great CD\n",
      "content: My lovely Pat has one of the GREAT voices of her generation. I have listened to this CD for YEARS and I still LOVE IT. When I'm in a good mood it makes me feel better. A bad mood just evaporates like sugar in the rain. This CD just oozes LIFE. Vocals are jusat STUUNNING and lyrics just kill. One of life's hidden gems. This is a desert isle CD in my book. Why she never made it big is just beyond me. Everytime I play this, no matter black, white, young, old, male, female EVERYBODY says one thing \"Who was that singing ?\"\n",
      "label: 1\n",
      "------------------------------\n",
      "title: One of the best game music soundtracks - for a game I didn't really play\n",
      "content: Despite the fact that I have only played a small portion of the game, the music I heard (plus the connection to Chrono Trigger which was great as well) led me to purchase the soundtrack, and it remains one of my favorite albums. There is an incredible mix of fun, epic, and emotional songs. Those sad and beautiful tracks I especially like, as there's not too many of those kinds of songs in my other video game soundtracks. I must admit that one of the songs (Life-A Distant Promise) has brought tears to my eyes on many occasions.My one complaint about this soundtrack is that they use guitar fretting effects in many of the songs, which I find distracting. But even if those weren't included I would still consider the collection worth it.\n",
      "label: 1\n",
      "------------------------------\n",
      "title: Batteries died within a year ...\n",
      "content: I bought this charger in Jul 2003 and it worked OK for a while. The design is nice and convenient. However, after about a year, the batteries would not hold a charge. Might as well just get alkaline disposables, or look elsewhere for a charger that comes with batteries that have better staying power.\n",
      "label: 0\n"
     ]
    }
   ],
   "source": [
    "#@title ðŸ” Quick look at the data { run: \"auto\" }\n",
    "#@markdown Lets have quick look at a few samples in our test set.\n",
    "n_samples_to_see = 3  #@param {type: \"integer\"}\n",
    "for i in range(n_samples_to_see):\n",
    "    print(\"-\" * 30)\n",
    "    print(\"title:\", dataset_test[i][\"title\"])\n",
    "    print(\"content:\", dataset_test[i][\"content\"])\n",
    "    print(\"label:\", dataset_test[i][\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzxwRDQFUtaG"
   },
   "source": [
    "### 1ï¸âƒ£ Tokenize the `text`\n",
    "Tokenize the `text`portion of each sample (i.e. parsing the text to smaller chunks). Tokenization can happen in many ways; traditionally, this was done based on the white spaces. With transformer-based models, tokenization is performed based on the frequency of occurrence of \"chunk of text\". This frequency can be learned in many different ways. However the most common one is the [**wordpiece**](https://arxiv.org/pdf/1609.08144v2.pdf) model. \n",
    "> The wordpiece model is generated using a data-driven approach to maximize the language-model likelihood\n",
    "of the training data, given an evolving word definition. Given a training corpus and a number of desired\n",
    "tokens $D$, the optimization problem is to select $D$ wordpieces such that the resulting corpus is minimal in the\n",
    "number of wordpieces when segmented according to the chosen wordpiece model.\n",
    "\n",
    "Under this model:\n",
    "1. Not all things can be converted to tokens depending on the model. For example, most models have been pretrained without any knowledge of emojis. So their token will be `[UNK]`, which stands for unknown.\n",
    "2. Some words will be mapped to multiple tokens!\n",
    "3. Depending on the kind of model, your tokens may or may not respect capitalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "qCpNwaTYSo3U"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f212211fc544411abc49dd55fa0dac58"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "11ddf5630a764ceb82b1156c46614021"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (â€¦)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2fa4af297e9248c6aef51984870170e5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (â€¦)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c4dbeb97db0746fd862b16c0782aac4d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3OMDqabyToBt",
    "outputId": "f3477698-69e0-41e6-c840-beca5796e5c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['welcome',\n 'to',\n 'if',\n '##t',\n '##6',\n '##13',\n '##5',\n '.',\n 'we',\n 'now',\n 'teach',\n 'you',\n '[UNK]',\n '(',\n 'hugging',\n 'face',\n ')',\n 'library',\n ':',\n 'dd',\n '##d',\n '.']"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title ðŸ” Quick look at tokenization { run: \"auto\", vertical-output: true }\n",
    "input_sample = \"Welcome to IFT6135. We now teach you ðŸ¤—(HUGGING FACE) Library :DDD.\"  #@param {type: \"string\"}\n",
    "tokenizer.tokenize(input_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEu6aqReXqp6"
   },
   "source": [
    "### 2ï¸âƒ£ Encoding\n",
    "Once we have tokenized the text, we then need to convert these chuncks to numbers so we can feed them to our model. This conversion is basically a look-up in a dictionary **from `str` $\\to$ `int`**. The tokenizer object can also perform this work. While it does so it will also add the *special* tokens needed by the model to the encodings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EpDGccrvYKnT",
    "outputId": "f5776da3-28fa-4fc2-d247-d231214395e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Token Encodings:\n",
      " [101, 6160, 2000, 2065, 2102, 2575, 17134, 2629, 1012, 2057, 2085, 6570, 2017, 100, 1006, 17662, 2227, 1007, 3075, 1024, 20315, 2094, 1012, 102]\n",
      "-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.\n",
      "--> Token Encodings Decoded:\n",
      " [CLS] welcome to ift6135. we now teach you [UNK] ( hugging face ) library : ddd. [SEP]\n"
     ]
    }
   ],
   "source": [
    "#@title ðŸ” Quick look at token encoding { run: \"auto\"}\n",
    "input_sample = \"Welcome to IFT6135. We now teach you ðŸ¤—(HUGGING FACE) Library :DDD.\"  #@param {type: \"string\"}\n",
    "\n",
    "print(\"--> Token Encodings:\\n\", tokenizer.encode(input_sample))\n",
    "print(\"-.\" * 15)\n",
    "print(\"--> Token Encodings Decoded:\\n\", tokenizer.decode(tokenizer.encode(input_sample)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DI8lFKZSZ2ZW"
   },
   "source": [
    "### 3ï¸âƒ£ Truncate/Pad samples\n",
    "Since all the sample in the batch will not have the same sequence length, we would need to truncate the longer sequences (i.e. the ones that exeed a predefined maximum length) and pad the shorter ones so we that we can equal length for all the samples in the batch. Once this is achieved, we would need to convert the result to `torch.Tensor`s and return. These tensors will then be retrieved from the [dataloader](https://https//pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "SP31MsbHZxp5"
   },
   "outputs": [],
   "source": [
    "class Collate:\n",
    "    def __init__(self, tokenizer: str, max_len: int) -> None:\n",
    "        self.tokenizer_name = tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __call__(self, batch: List[Dict[str, Union[str, int]]]) -> Dict[str, torch.Tensor]:\n",
    "        texts = list(map(lambda batch_instance: batch_instance[\"title\"], batch))\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            texts,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "            return_token_type_ids=False,\n",
    "        )\n",
    "\n",
    "        labels = list(map(lambda batch_instance: int(batch_instance[\"label\"]), batch))\n",
    "        labels = torch.LongTensor(labels)\n",
    "        return dict(tokenized_inputs, **{\"labels\": labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "4VaSpuyIjNqn"
   },
   "outputs": [],
   "source": [
    "#@title ðŸ§‘â€ðŸ³ Setting up the collate function { run: \"auto\" }\n",
    "tokenizer_name = \"bert-base-uncased\"  #@param {type: \"string\"}\n",
    "sample_max_length = 256  #@param {type:\"slider\", min:32, max:512, step:1}\n",
    "collate = Collate(tokenizer=tokenizer_name, max_len=sample_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "y9P4oWyOSexA"
   },
   "outputs": [],
   "source": [
    "class ReviewClassifier(nn.Module):\n",
    "    def __init__(self, backbone: str, backbone_hidden_size: int, nb_classes: int):\n",
    "        super(ReviewClassifier, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.backbone_hidden_size = backbone_hidden_size\n",
    "        self.nb_classes = nb_classes\n",
    "        self.back_bone = AutoModel.from_pretrained(\n",
    "            self.backbone,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "        )\n",
    "        self.classifier = torch.nn.Linear(self.backbone_hidden_size, self.nb_classes)\n",
    "\n",
    "    def forward(\n",
    "            self, input_ids: torch.Tensor, attention_mask: torch.Tensor, labels: Optional[torch.Tensor] = None\n",
    "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        back_bone_output = self.back_bone(input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = back_bone_output[0]\n",
    "        pooled_output = hidden_states[:, 0]  # getting the [CLS] token\n",
    "        logits = self.classifier(pooled_output)\n",
    "        if labels is not None:\n",
    "            loss_fn = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(\n",
    "                logits.view(-1, self.nb_classes),\n",
    "                labels.view(-1),\n",
    "            )\n",
    "            return loss, logits\n",
    "        return logits\n",
    "\n",
    "\n",
    "class ReviewClassifierLSTM(nn.Module):\n",
    "    def __init__(self, nb_classes: int, encoder_only: bool = False, dropout=0.5):\n",
    "        super(ReviewClassifierLSTM, self).__init__()\n",
    "        self.nb_classes = nb_classes\n",
    "        self.encoder_only = encoder_only\n",
    "        self.back_bone = EncoderDecoder(dropout=dropout, encoder_only=encoder_only)\n",
    "        self.classifier = torch.nn.Linear(256, self.nb_classes)\n",
    "\n",
    "    def forward(\n",
    "            self, input_ids: torch.Tensor, attention_mask: torch.Tensor, labels: Optional[torch.Tensor] = None\n",
    "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        hidden_states, _ = self.back_bone(input_ids, attention_mask)\n",
    "        pooled_output = hidden_states\n",
    "        logits = self.classifier(pooled_output)\n",
    "        if labels is not None:\n",
    "            loss_fn = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(\n",
    "                logits.view(-1, self.nb_classes),\n",
    "                labels.view(-1),\n",
    "            )\n",
    "            return loss, logits\n",
    "        return logits\n",
    "\n",
    "\n",
    "class ReviewClassifierTransformer(nn.Module):\n",
    "    def __init__(self, nb_classes: int, num_heads: int = 4, num_layers: int = 4, block: str = \"prenorm\", dropout: float = 0.3):\n",
    "        super(ReviewClassifierTransformer, self).__init__()\n",
    "        self.nb_classes = nb_classes\n",
    "        self.back_bone = Transformer(num_heads=num_heads, num_layers=num_layers, block=block, dropout=dropout)\n",
    "        self.classifier = torch.nn.Linear(256, self.nb_classes)\n",
    "\n",
    "    def forward(\n",
    "            self, input_ids: torch.Tensor, attention_mask: torch.Tensor, labels: Optional[torch.Tensor] = None\n",
    "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        back_bone_output = self.back_bone(input_ids, attention_mask)\n",
    "        hidden_states = back_bone_output\n",
    "        pooled_output = hidden_states\n",
    "        logits = self.classifier(pooled_output)\n",
    "        if labels is not None:\n",
    "            loss_fn = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(\n",
    "                logits.view(-1, self.nb_classes),\n",
    "                labels.view(-1),\n",
    "            )\n",
    "            return loss, logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "HP58LrWUjFt4"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"--> Device selected: {device}\")\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "        model: torch.nn.Module, training_data_loader: DataLoader, optimizer: torch.optim.Optimizer, logging_frequency: int, testing_data_loader: DataLoader, logger: dict):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    epoch_loss = 0\n",
    "    logging_loss = 0\n",
    "    start_time = time.time()\n",
    "    mini_start_time = time.time()\n",
    "    for step, batch in enumerate(training_data_loader):\n",
    "        batch = {key: value.to(device) for key, value in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        logging_loss += loss.item()\n",
    "\n",
    "        if (step + 1) % logging_frequency == 0:\n",
    "            freq_time = time.time() - mini_start_time\n",
    "            logger['train_time'].append(freq_time + logger['train_time'][-1])\n",
    "            logger['train_losses'].append(logging_loss / logging_frequency)\n",
    "            print(f\"Training loss @ step {step + 1}: {logging_loss / logging_frequency}\")\n",
    "            eval_acc, eval_f1, eval_loss, eval_time = evaluate(model, testing_data_loader)\n",
    "            logger['eval_accs'].append(eval_acc)\n",
    "            logger['eval_f1s'].append(eval_f1)\n",
    "            logger['eval_losses'].append(eval_loss)\n",
    "            logger['eval_time'].append(eval_time + logger['eval_time'][-1])\n",
    "\n",
    "            logging_loss = 0\n",
    "            mini_start_time = time.time()\n",
    "\n",
    "    return epoch_loss / len(training_data_loader), time.time() - start_time\n",
    "\n",
    "\n",
    "def evaluate(model: torch.nn.Module, test_data_loader: DataLoader):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    eval_loss = 0\n",
    "    correct_predictions = {i: 0 for i in range(2)}\n",
    "    total_predictions = {i: 0 for i in range(2)}\n",
    "    preds = []\n",
    "    targets = []\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(test_data_loader):\n",
    "            batch = {key: value.to(device) for key, value in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs[0]\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            predictions = np.argmax(outputs[1].detach().cpu().numpy(), axis=1)\n",
    "            preds.extend(predictions.tolist())\n",
    "            targets.extend(batch[\"labels\"].cpu().numpy().tolist())\n",
    "\n",
    "            for target, prediction in zip(batch[\"labels\"].cpu().numpy(), predictions):\n",
    "                if target == prediction:\n",
    "                    correct_predictions[target] += 1\n",
    "                total_predictions[target] += 1\n",
    "    accuracy = (100.0 * sum(correct_predictions.values())) / sum(total_predictions.values())\n",
    "    f1 = f1_score(targets, preds)\n",
    "    model.train()\n",
    "    return accuracy, round(f1, 4), eval_loss / len(test_data_loader), time.time() - start_time\n",
    "\n",
    "\n",
    "def save_logs(dictionary, log_dir, exp_id):\n",
    "    log_dir = os.path.join(log_dir, exp_id)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    # Log arguments\n",
    "    with open(os.path.join(log_dir, \"args.json\"), \"w\") as f:\n",
    "        json.dump(dictionary, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Device selected: cuda\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Module [ModuleList] is missing the required \"forward\" function",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 117\u001B[0m\n\u001B[1;32m    114\u001B[0m logger[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mparameters\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m([p\u001B[38;5;241m.\u001B[39mnumel() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m model\u001B[38;5;241m.\u001B[39mback_bone\u001B[38;5;241m.\u001B[39mparameters() \u001B[38;5;28;01mif\u001B[39;00m p\u001B[38;5;241m.\u001B[39mrequires_grad])\n\u001B[1;32m    115\u001B[0m nb_epoch\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[0;32m--> 117\u001B[0m train_loss, train_time \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_one_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogging_frequency\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogger\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    118\u001B[0m eval_acc, eval_f1, eval_loss, eval_time  \u001B[38;5;241m=\u001B[39m evaluate(model, test_loader)\n\u001B[1;32m    119\u001B[0m logger[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtotal_train_loss\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m train_loss\n",
      "Cell \u001B[0;32mIn[19], line 14\u001B[0m, in \u001B[0;36mtrain_one_epoch\u001B[0;34m(model, training_data_loader, optimizer, logging_frequency, testing_data_loader, logger)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step, batch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(training_data_loader):\n\u001B[1;32m     13\u001B[0m     batch \u001B[38;5;241m=\u001B[39m {key: value\u001B[38;5;241m.\u001B[39mto(device) \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m batch\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m---> 14\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m     loss \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     16\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m/usr/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn[16], line 70\u001B[0m, in \u001B[0;36mReviewClassifierTransformer.forward\u001B[0;34m(self, input_ids, attention_mask, labels)\u001B[0m\n\u001B[1;32m     67\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;28mself\u001B[39m, input_ids: torch\u001B[38;5;241m.\u001B[39mTensor, attention_mask: torch\u001B[38;5;241m.\u001B[39mTensor, labels: Optional[torch\u001B[38;5;241m.\u001B[39mTensor] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     69\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Union[torch\u001B[38;5;241m.\u001B[39mTensor, Tuple[torch\u001B[38;5;241m.\u001B[39mTensor, torch\u001B[38;5;241m.\u001B[39mTensor]]:\n\u001B[0;32m---> 70\u001B[0m     back_bone_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mback_bone\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     71\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m back_bone_output\n\u001B[1;32m     72\u001B[0m     pooled_output \u001B[38;5;241m=\u001B[39m hidden_states\n",
      "File \u001B[0;32m/usr/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/IFT6135/Devoir_2/transformer_solution.py:394\u001B[0m, in \u001B[0;36mTransformer.forward\u001B[0;34m(self, x, mask)\u001B[0m\n\u001B[1;32m    391\u001B[0m x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpos_embedding[:, :T \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m    392\u001B[0m \u001B[38;5;66;03m# Add dropout and then the transformer\u001B[39;00m\n\u001B[1;32m    393\u001B[0m \u001B[38;5;66;03m# ==========================\u001B[39;00m\n\u001B[0;32m--> 394\u001B[0m x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransformer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    395\u001B[0m \u001B[38;5;66;03m# ==========================\u001B[39;00m\n\u001B[1;32m    396\u001B[0m \n\u001B[1;32m    397\u001B[0m \u001B[38;5;66;03m# Take the cls token representation and send it to mlp_head\u001B[39;00m\n\u001B[1;32m    398\u001B[0m \n\u001B[1;32m    399\u001B[0m \u001B[38;5;66;03m# ==========================\u001B[39;00m\n\u001B[1;32m    400\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmlp_head(x)\n",
      "File \u001B[0;32m/usr/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/usr/lib/python3.10/site-packages/torch/nn/modules/module.py:363\u001B[0m, in \u001B[0;36m_forward_unimplemented\u001B[0;34m(self, *input)\u001B[0m\n\u001B[1;32m    352\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_forward_unimplemented\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    353\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Defines the computation performed at every call.\u001B[39;00m\n\u001B[1;32m    354\u001B[0m \n\u001B[1;32m    355\u001B[0m \u001B[38;5;124;03m    Should be overridden by all subclasses.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    361\u001B[0m \u001B[38;5;124;03m        registered hooks while the latter silently ignores them.\u001B[39;00m\n\u001B[1;32m    362\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 363\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModule [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m] is missing the required \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;124mforward\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;124m function\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mNotImplementedError\u001B[0m: Module [ModuleList] is missing the required \"forward\" function"
     ]
    }
   ],
   "source": [
    "nb_epoch = 100\n",
    "batch_size = 512\n",
    "logging_frequency = 5\n",
    "learning_rate = 1e-5\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "for i in range(1, 7):\n",
    "    experimental_setting = i\n",
    "    # 4 experimental settings\n",
    "\n",
    "    if experimental_setting == 1:\n",
    "        model = ReviewClassifierLSTM(nb_classes=2, dropout=0.3, encoder_only=True)\n",
    "    if experimental_setting == 2:\n",
    "        model = ReviewClassifierLSTM(nb_classes=2, dropout=0.3, encoder_only=False)\n",
    "    if experimental_setting == 3:\n",
    "        model = ReviewClassifierTransformer(nb_classes=2, num_heads=4, num_layers=2, block='prenorm', dropout=0.3)\n",
    "    if experimental_setting == 4:\n",
    "        model = ReviewClassifierTransformer(nb_classes=2, num_heads=4, num_layers=4, block='prenorm', dropout=0.3)\n",
    "    if experimental_setting == 5:\n",
    "        model = ReviewClassifierTransformer(nb_classes=2, num_heads=4, num_layers=2, block='postnorm', dropout=0.3)\n",
    "    if experimental_setting == 6:\n",
    "        model = ReviewClassifier(backbone=\"bert-base-uncased\", backbone_hidden_size=768, nb_classes=2)\n",
    "        for parameter in model.back_bone.parameters():\n",
    "            parameter.requires_grad = False\n",
    "        logging_frequency = 703\n",
    "\n",
    "    # setting up the optimizer\n",
    "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate, eps=1e-8)\n",
    "    model.to(device)\n",
    "    logger = dict()\n",
    "    logger['train_time'] = [0]\n",
    "    logger['eval_time'] = [0]\n",
    "    logger['train_losses'] = []\n",
    "    logger['eval_accs'] = []\n",
    "    logger['eval_f1s'] = []\n",
    "    logger['eval_losses'] = []\n",
    "\n",
    "    logger['parameters'] = sum([p.numel() for p in model.back_bone.parameters() if p.requires_grad])\n",
    "    nb_epoch = 1\n",
    "\n",
    "    train_loss, train_time = train_one_epoch(model, train_loader, optimizer, logging_frequency, test_loader, logger)\n",
    "    eval_acc, eval_f1, eval_loss, eval_time = evaluate(model, test_loader)\n",
    "    logger[\"total_train_loss\"] = train_loss\n",
    "    logger[\"total_train_time\"] = train_time\n",
    "    logger[\"final_eval_loss\"] = eval_loss\n",
    "    logger[\"final_eval_time\"] = eval_time\n",
    "    logger[\"final_eval_acc\"] = eval_acc\n",
    "    logger[\"final_eval_f1\"] = eval_f1\n",
    "    logger['train_time'] = logger['train_time'][1:]\n",
    "    logger['eval_time'] = logger['eval_time'][1:]\n",
    "\n",
    "    print(f\"    Epoch: {1} Loss/Test: {eval_loss}, Loss/Train: {train_loss}, Acc/Test: {eval_acc}, F1/Test: {eval_f1}, Train Time: {train_time}, Eval Time: {eval_time}\")\n",
    "    save_logs(logger, \"assignment/log\", str(experimental_setting))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy\n",
    "import matplotlib\n",
    "matplotlib.use(\"tkagg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "\n",
    "from typing import List, Dict, Union, Optional, Tuple\n",
    "import torch\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def load_logs(log_dir, exp_id):\n",
    "    log_dir = os.path.join(log_dir, exp_id)\n",
    "    with open(os.path.join(log_dir, \"args.json\"), \"r\") as f:\n",
    "        dictionary = json.load(f)\n",
    "    return dictionary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "train_losses, eval_losses, eval_accs = [], [], []\n",
    "total_train_time, final_eval_time = [], []\n",
    "for i in range(1, 7):\n",
    "    experimental_setting = i\n",
    "    dictionary = load_logs(\"assignment/log\", str(experimental_setting))\n",
    "    train_losses.append(dictionary[\"train_losses\"])\n",
    "    eval_losses.append(dictionary[\"eval_losses\"])\n",
    "    eval_accs.append(dictionary[\"eval_accs\"])\n",
    "\n",
    "    total_train_time.append(dictionary[\"total_train_time\"])\n",
    "    final_eval_time.append(dictionary[\"final_eval_time\"])\n",
    "    # print(type(dictionary))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "for i in range(0, 6):\n",
    "    matplotlib.pyplot.figure(\"train_losses\")\n",
    "    matplotlib.pyplot.plot(train_losses[i], label=f\"experiment {i+1}\")\n",
    "    matplotlib.pyplot.legend()\n",
    "    matplotlib.pyplot.xlabel(\"training iteration\")\n",
    "    matplotlib.pyplot.ylabel(\"training loss\")\n",
    "    matplotlib.pyplot.savefig(\"assignment/figure/train_losses.pdf\")\n",
    "\n",
    "    matplotlib.pyplot.figure(\"eval_losses\")\n",
    "    matplotlib.pyplot.plot(eval_losses[i], label=f\"experiment {i+1}\")\n",
    "    matplotlib.pyplot.legend()\n",
    "    matplotlib.pyplot.xlabel(\"training iteration\")\n",
    "    matplotlib.pyplot.ylabel(\"validation loss\")\n",
    "    matplotlib.pyplot.savefig(\"assignment/figure/eval_losses.pdf\")\n",
    "\n",
    "    matplotlib.pyplot.figure(\"eval_accs\")\n",
    "    matplotlib.pyplot.plot(eval_accs[i], label=f\"experiment {i+1}\")\n",
    "    matplotlib.pyplot.legend()\n",
    "    matplotlib.pyplot.xlabel(\"training iteration\")\n",
    "    matplotlib.pyplot.ylabel(\"validation accuracy\")\n",
    "    matplotlib.pyplot.savefig(\"assignment/figure/eval_accs.pdf\")\n",
    "\n",
    "matplotlib.pyplot.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# arch. best_train_lost best_val._loss best_val._acc. tot_train_time eval_time\n",
      "1 LSTM/E(only) 0.343 0.409 81.200 986 0.110\n",
      "2 LSTM/E/D 0.313 0.371 83.300 1487 0.151\n",
      "3 Transformer/2L/pre 0.421 0.454 78.700 2502 0.269\n",
      "4 Transformer/4L/pre 0.420 0.436 79.700 4527 0.397\n",
      "5 Transformer/2L/post 0.429 0.456 78.500 2412 0.232\n",
      "6 BERT 0.420 0.425 80.900 30362 5.843\n"
     ]
    }
   ],
   "source": [
    "print(f\"# arch. best_train_lost best_val._loss best_val._acc. tot_train_time eval_time\")\n",
    "# print(f\"# arch. best_train_lost best_val._loss best_val._acc. tot_train_time eval_time\")\n",
    "\n",
    "architecture_name = [\"LSTM/E(only)\", \"LSTM/E/D\", \"Transformer/2L/pre\", \"Transformer/4L/pre\", \"Transformer/2L/post\", \"BERT\"]\n",
    "# architecture_name = [\"LSTM/E(only)\", \"LSTM/E/D\", \"Transf./2/pre\", \"Transf./4/pre\", \"Transf./2/post\", \"BERT\"]\n",
    "\n",
    "for i in range(0, 6):\n",
    "    best_tl, best_vl, best_va = numpy.min(train_losses[i]), numpy.min(eval_losses[i]), numpy.max(eval_accs[i])\n",
    "\n",
    "    print(f\"{i+1} {architecture_name[i]} {best_tl:.3f} {best_vl:.3f} {best_va:.3f} {total_train_time[i]:.0f} {final_eval_time[i]:.3f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment architecture_name best_train_lost best_valid_loss best_valid_accuracy total_train_time final_eval_time\n",
      "1 LSTM/True                 0.343 0.409 81.200    937.284 0.106\n",
      "2 LSTM/False                0.315 0.371 83.600   1424.998 0.147\n",
      "3 Transformer/2/prenorm     0.445 0.474 77.700   2421.104 0.242\n",
      "4 Transformer/4/prenorm     0.683 0.684 62.900   4503.754 0.413\n",
      "5 Transformer/2/postnorm    0.479 0.496 76.500   2411.199 0.244\n",
      "6 BERT                      0.420 0.425 80.900  30305.980 5.805\n"
     ]
    }
   ],
   "source": [
    "print(f\"experiment architecture_name best_train_lost best_valid_loss best_valid_accuracy total_train_time final_eval_time\")\n",
    "architecture_name = [\"LSTM/encoder_only\", \"LSTM/encoder/decoder\", \"Transformer/2 heads/prenorm\", \"Transformer/4 heads/prenorm\", \"Transformer/2 heads/postnorm\", \"BERT\"]\n",
    "# architecture_name = [\"LSTM/encoder_only\", \"LSTM/encoder/decoder\", \"Transformer/2 heads/prenorm\", \"Transformer/4 heads/prenorm\", \"Transformer/2 heads/postnorm\", \"BERT\"]\n",
    "\n",
    "for i in range(0, 6):\n",
    "    best_tl, best_vl, best_va = numpy.min(train_losses[i]), numpy.min(eval_losses[i]), numpy.max(eval_accs[i])\n",
    "\n",
    "    print(f\"{i+1} {architecture_name[i]:25} {best_tl:.3f} {best_vl:.3f} {best_va:.3f} {total_train_time[i]:10.3f} {final_eval_time[i]:.3f}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
